help("AMADA")
help(package="AMADA")
N = 10
M= 3
matrix(rnorm(N*M,mean=0,sd=1), N, M)
N = 100
M= 1000
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
M1
N = 10000
M= 1000
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"phylogram")
corr
Fig1<-plotdendrogram(corr,"fan")
N = 5000
M= 100
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"fan")
N = 5000
M= 500
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"fan")
N = 5000
M= 100
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"fan")
N = 50000
M= 100
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"fan")
N = 100000
M= 100
M1<-matrix(rnorm(N*M,mean=0,sd=1), N, M)
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"fan")
ptm <- proc.time()
corr<-Corr_MIC(M1,"pearson")
Fig1<-plotdendrogram(corr,"fan")
proc.time() - ptm
1.432+0.044
help(package="AMADA")
install.packages("GGally")
library("GGally", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
data(diamonds, package="ggplot2")
diamonds.samp <- diamonds[sample(1:dim(diamonds)[1],200),]
# Custom Example
pm <- ggpairs(
diamonds.samp[,1:3],
upper = list(continuous = "density", combo = "box"),
lower = list(continuous = "points", combo = "dot"),
color = "cut",
title = "Diamonds"
)
pm
pm <- ggpairs(
diamonds.samp[,1:3],
upper = list(continuous = "density", combo = "box"),
lower = list(continuous = "density", combo = "dot"),
color = "cut",
title = "Diamonds"
)
pm
ggpairs(
diamonds.samp[,1:3],
upper = list(continuous = "density", combo = "box"),
lower = list(continuous = "density"),
color = "cut",
title = "Diamonds"
)
ggpairs(
diamonds.samp[,1:3],
upper = list(continuous = "density"),
lower = list(continuous = "density"),
color = "cut",
title = "Diamonds"
)
ggpairs(
iris[,1:4],
upper = list(continuous = "density"),
lower = list(continuous = "density"),
color = "cut",
title = "Diamonds"
)
ggpairs(
iris[,1:4],
upper = list(continuous = "density"),
lower = list(continuous = "density")
)
ggpairs(
iris[,1:4],
upper = list(continuous = "cor"),
lower = list(continuous = "density")
)
pm <- ggpairs(
iris[,1:4],
upper = list(continuous = "cor"),
lower = list(continuous = "density"),
diag=list("density")
)
pm
ggpairs(
iris[,1:4],
upper = list(continuous = "cor"),
lower = list(continuous = "density"),
diag=list("bar'")
)
pm <- ggpairs(
iris[,1:4],
diag=list("bar"),
upper = list(continuous = "cor"),
lower = list(continuous = "density")
)
pm
pm <- ggpairs(
iris[,1:4],
diag=list("bar"),
upper = list(continuous = "cor"),
lower = list(continuous = "hist")
)
pm
pm <- ggpairs(
iris[,1:4],
diag=list("continuous"),
upper = list(continuous = "cor"),
lower = list(continuous = "density")
)
pm
install.packages("R1magic")
library(R1magic)#  Signal components
N <- 100
# Sparse components
K <- 4
#  Up to Measurements  > K LOG (N/K)
M <- 40
# Measurement Matrix (Random Sampling Sampling)
phi <- GaussianMatrix(N,M)
# R1magic generate random signal
xorg <- sparseSignal(N, K, nlev=1e-3)
y <- phi %*% xorg ;# generate measurement
T <- diag(N) ;# Do identity transform
p <- matrix(0, N, 1) ;# initial guess
# R1magic Convex Minimization ! (unoptimized-parameter)
ll <- solveL1(phi, y, T, p)
x1 <- ll$estimate
plot( 1:100, seq(0.011,1.1,0.011), type = “n”,xlab=””,ylab=””)
title(main=”Random Sparse Signal Recovery”,
xlab=”Signal Component”,ylab=”Spike Value”)
lines(1:100, xorg , col = “red”)
lines(1:100, x1, col = “blue”, cex = 1.5)
# shifted by 5 for clearity
library(R1magic)#  Signal components
N <- 100
# Sparse components
K <- 4
#  Up to Measurements  > K LOG (N/K)
M <- 40
# Measurement Matrix (Random Sampling Sampling)
phi <- GaussianMatrix(N,M)
# R1magic generate random signal
xorg <- sparseSignal(N, K, nlev=1e-3)
y <- phi %*% xorg ;# generate measurement
T <- diag(N) ;# Do identity transform
p <- matrix(0, N, 1) ;# initial guess
# R1magic Convex Minimization ! (unoptimized-parameter)
ll <- solveL1(phi, y, T, p)
x1 <- ll$estimate
plot( 1:100, seq(0.011,1.1,0.011), type = "n",xlab="",ylab="")
title(main="Random Sparse Signal Recovery",
xlab="Signal Component",ylab="Spike Value")
lines(1:100, xorg , col = "red")
lines(1:100, x1, col = “blue”, cex = 1.5)
# shifted by 5 for clearity
library(R1magic)#  Signal components
N <- 100
# Sparse components
K <- 4
#  Up to Measurements  > K LOG (N/K)
M <- 40
# Measurement Matrix (Random Sampling Sampling)
phi <- GaussianMatrix(N,M)
# R1magic generate random signal
xorg <- sparseSignal(N, K, nlev=1e-3)
y <- phi %*% xorg ;# generate measurement
T <- diag(N) ;# Do identity transform
p <- matrix(0, N, 1) ;# initial guess
# R1magic Convex Minimization ! (unoptimized-parameter)
ll <- solveL1(phi, y, T, p)
x1 <- ll$estimate
plot( 1:100, seq(0.011,1.1,0.011), type = "n",xlab="",ylab="")
title(main="Random Sparse Signal Recovery",
xlab="Signal Component",ylab="Spike Value")
lines(1:100, xorg , col = "red")
lines(1:100, x1, col = "blue", cex = 1.5)
xorg
x1
phi
y
diag(N)
matrix(0, N, 1)
5.226/2.760
1.893478*0.74
require(rstan)
# Set Seed
set.seed(1234)
# Number of Households
H=100
# True parameters
true.b.mu = c(10,-5)
true.b.sig = c(3,2)
true.y.sig = 3
# Storage
b =  matrix(0,100,2)
ctr = 0
# Each "household" can have between 5 and 10 reps
Ti = sample(5:10,H,replace=T)
id = x = y = matrix(0,sum(Ti),1)
# Simulate Data
for(i in 1:100) {
b[i,1] = rnorm(1,true.b.mu[1],true.b.sig[1])
b[i,2] = rnorm(1,true.b.mu[2],true.b.sig[2])
for(j in 1:Ti[i]) {
ctr = ctr + 1
x[ctr] = runif(1)*3 + 1
y[ctr] = b[i,1]+b[i,2]*x[ctr]+rnorm(1,0,true.y.sig)
id[ctr] = i
}
}
data {
int<lower=0> N; // Observations
int<lower=0> H; // persons
int<lower=0> id[N]; // ID variable
real y[N]; // Y Vector
real x[N]; // X Vector
}
data ="{
int<lower=0> N; // Observations
int<lower=0> H; // persons
int<lower=0> id[N]; // ID variable
real y[N]; // Y Vector
real x[N]; // X Vector
}"
parameters=" {
vector[2] beta[H];	// [H,2] dim matrix for b
real beta_mu[2]; // means for b
real<lower=0> beta_sig[2];   // var parameters for b
real<lower=0> y_sig;   // regression variance
}""
hcode = "
data {
int<lower=0> N; // Observations
int<lower=0> H; // persons
int<lower=0> id[N]; // ID variable
real y[N]; // Y Vector
real x[N]; // X Vector
}
parameters {
vector[2] beta[H];	// [H,2] dim matrix for b
real beta_mu[2]; // means for b
real<lower=0> beta_sig[2];   // var parameters for b
real<lower=0> y_sig;   // regression variance
}
model {
//Priors
y_sig~uniform(0,1000);
for(j in 1:2) {
beta_mu[j] ~ normal(0,100);
beta_sig[j] ~ uniform(0,1000);
for(h in 1:H)
beta[h,j] ~ normal(beta_mu[j], beta_sig[j]);
}
// Likelihood
for(n in 1:N)
y[n] ~ normal(beta[id[n],1]+beta[id[n],2] .* x[n], y_sig);
}
"
data ="{
int<lower=0> N; // Observations
int<lower=0> H; // persons
int<lower=0> id[N]; // ID variable
real y[N]; // Y Vector
real x[N]; // X Vector
}"
parameters=" {
vector[2] beta[H];	// [H,2] dim matrix for b
real beta_mu[2]; // means for b
real<lower=0> beta_sig[2];   // var parameters for b
real<lower=0> y_sig;   // regression variance
}"
model = "{
#Priors
y_sig~uniform(0,1000);
for(j in 1:2) {
beta_mu[j] ~ normal(0,100);
beta_sig[j] ~ uniform(0,1000);
for(h in 1:H)
beta[h,j] ~ normal(beta_mu[j], beta_sig[j]);
}
# Likelihood
for(n in 1:N)
y[n] ~ normal(beta[id[n],1]+beta[id[n],2] .* x[n], y_sig);
}"
dat = list(N=length(y),H=dim(b)[1],y=y,x=x,id=id)
hlm = stan(model_name="Hierarchical Linear Model", model_code = hcode, data=dat , iter = 2000, n_chains = 2, verbose = FALSE)
hcode
hcode = "
data {
int<lower=0> N; // Observations
int<lower=0> H; // persons
int<lower=0> id[N]; // ID variable
real y[N]; // Y Vector
real x[N]; // X Vector
}
parameters {
vector[2] beta[H];	// [H,2] dim matrix for b
real beta_mu[2]; // means for b
real<lower=0> beta_sig[2];   // var parameters for b
real<lower=0> y_sig;   // regression variance
}
model {
//Priors
y_sig~uniform(0,1000);
for(j in 1:2) {
beta_mu[j] ~ normal(0,100);
beta_sig[j] ~ uniform(0,1000);
for(h in 1:H)
beta[h,j] ~ normal(beta_mu[j], beta_sig[j]);
}
// Likelihood
for(n in 1:N)
y[n] ~ normal(beta[id[n],1]+beta[id[n],2] .* x[n], y_sig);
}
"
hcode
data ="{
int<lower=0> N; // Observations
int<lower=0> H; // persons
int<lower=0> id[N]; // ID variable
real y[N]; // Y Vector
real x[N]; // X Vector
}"
parameters=" {
vector[2] beta[H];	// [H,2] dim matrix for b
real beta_mu[2]; // means for b
real<lower=0> beta_sig[2];   // var parameters for b
real<lower=0> y_sig;   // regression variance
}"
model = "{
#Priors
y_sig~uniform(0,1000);
for(j in 1:2) {
beta_mu[j] ~ normal(0,100);
beta_sig[j] ~ uniform(0,1000);
for(h in 1:H)
beta[h,j] ~ normal(beta_mu[j], beta_sig[j]);
}
# Likelihood
for(n in 1:N)
y[n] ~ normal(beta[id[n],1]+beta[id[n],2] .* x[n], y_sig);
}"
dat = list(N=length(y),H=dim(b)[1],y=y,x=x,id=id)
hlm = stan(model_name="Hierarchical Linear Model", model_code = hcode, data=dat , iter = 2000, n_chains = 2, verbose = FALSE)
setwd("~/Dropbox/artigos/Meusartigos/IAA-WGC/Github/Logit_Mar")
# JAGS Code with Adaptive Shrinkage
# Read and format data
data<-read.csv("sample_agn.csv",header=TRUE,na.strings="")
data2<-na.omit(data)
data2<-data2[data2$logMstar>0,]
data2<-data2[which(data2$vlos_sigma!=Inf),]
#data2$bpt<-as.factor(data2$bpt)
data2$bpt <- revalue(data2$bpt,c("SF"="0","Composite"="0",
"LINER"="1","Seyfert/LINER"="1",
"Seyfert"="1","BLANK"="0"))
# Prepare data for JAGS
data2$logMstar<-(data2$logMstar-mean(data2$logMstar))/sd(data2$logMstar)
data2$logMhalo<-(data2$logMhalo-mean(data2$logMhalo))/sd(data2$logMhalo)
data2$vlos_sigma<-(data2$vlos_sigma-mean(data2$vlos_sigma))/sd(data2$vlos_sigma)
data2$r_rvir<-(data2$r_rvir-mean(data2$r_rvir))/sd(data2$r_rvir)
X<-model.matrix(~logMstar+logMhalo+vlos_sigma+r_rvir,data=data2)
K<-ncol(X)
jags.data <- list(Y= data2$bpt,
N = nrow(data2),
X=X,
b0 = rep(0,K),
B0=diag(1e-4,K),
Npred = K
)
model<-"model{
#1. Priors
#beta~dmnorm(b0[],B0[,]) # Normal Priors
# Jefreys priors for sparseness
for(j in 1:Npred)   {
lnTau[j] ~ dunif(-50, 50)
TauM[j] <- exp(lnTau[j])
beta[j] ~ dnorm(0, TauM[j])
Ind[j] <- step(abs(beta[j]) - 0.05)
}
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
#3. Prediction
NewPred[i]~dbern(pi[i])
}
}"
params <- c("beta","pi","Ind","NewPred")
inits0  <- function () {
list(beta = rnorm(K, 0, 0.1))}
inits1=inits0()
inits2=inits0()
inits3=inits0()
library(parallel)
cl <- makeCluster(3)
jags.logit <- run.jags(method="rjparallel",
data = jags.data,
inits = list(inits1,inits2,inits3),
model=model,
n.chains = 3,
adapt=1000,
monitor=c(params),
burnin=5000,
sample=15000,
summarise=FALSE,
plots=FALSE
)
jagssamples <- as.mcmc.list(jags.logit)
# JAGS Code with Adaptive Shrinkage
#  Required libraries
library(rjags)
library(ggmcmc)
library(ggplot2)
library(ggthemes)
library(pander)
library(Cairo)
library(MASS)
library(scales)
library(plyr)
require(gdata)
require(runjags)
require(gdata)
require(caret)
require(pROC)
require(plyr)
# Read and format data
data<-read.csv("sample_agn.csv",header=TRUE,na.strings="")
data2<-na.omit(data)
data2<-data2[data2$logMstar>0,]
data2<-data2[which(data2$vlos_sigma!=Inf),]
#data2$bpt<-as.factor(data2$bpt)
data2$bpt <- revalue(data2$bpt,c("SF"="0","Composite"="0",
"LINER"="1","Seyfert/LINER"="1",
"Seyfert"="1","BLANK"="0"))
# Prepare data for JAGS
data2$logMstar<-(data2$logMstar-mean(data2$logMstar))/sd(data2$logMstar)
data2$logMhalo<-(data2$logMhalo-mean(data2$logMhalo))/sd(data2$logMhalo)
data2$vlos_sigma<-(data2$vlos_sigma-mean(data2$vlos_sigma))/sd(data2$vlos_sigma)
data2$r_rvir<-(data2$r_rvir-mean(data2$r_rvir))/sd(data2$r_rvir)
X<-model.matrix(~logMstar+logMhalo+vlos_sigma+r_rvir,data=data2)
K<-ncol(X)
jags.data <- list(Y= data2$bpt,
N = nrow(data2),
X=X,
b0 = rep(0,K),
B0=diag(1e-4,K),
Npred = K
)
model<-"model{
#1. Priors
#beta~dmnorm(b0[],B0[,]) # Normal Priors
# Jefreys priors for sparseness
for(j in 1:Npred)   {
lnTau[j] ~ dunif(-50, 50)
TauM[j] <- exp(lnTau[j])
beta[j] ~ dnorm(0, TauM[j])
Ind[j] <- step(abs(beta[j]) - 0.05)
}
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
#3. Prediction
NewPred[i]~dbern(pi[i])
}
}"
params <- c("beta","pi","Ind","NewPred")
inits0  <- function () {
list(beta = rnorm(K, 0, 0.1))}
inits1=inits0()
inits2=inits0()
inits3=inits0()
library(parallel)
cl <- makeCluster(3)
jags.logit <- run.jags(method="rjparallel",
data = jags.data,
inits = list(inits1,inits2,inits3),
model=model,
n.chains = 3,
adapt=1000,
monitor=c(params),
burnin=5000,
sample=15000,
summarise=FALSE,
plots=FALSE
)
jagssamples <- as.mcmc.list(jags.logit)
