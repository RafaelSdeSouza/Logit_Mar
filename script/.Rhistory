#c.LASSO
#for(j in 1:K){
#beta[j]~ddexp(0,tau)
#}
#c1.prior for tau
#tau <-pow(sdBeta,-1)
#sdBeta ~ dgamma(0.01,0.01)
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i,1:J])
for (j in 1:J){
#logit(pi[i,j]) <-  eta[i]
#eta[i] <- inprod(beta[galtype[i]], X[i,])
logit(pi[i,j]) <- beta[1,j]+beta[2,j]*X[i,2]+beta[3,j]*X[i,3]+beta[4,j]*X[i,4]+beta[5,j]*X[i,5]+beta[6,j]*X[i,6]+
beta[7,j]*X[i,7]
}   # close J loop
}     # close N loop
#3.Probability for each variable
# E galaxies
#for(l in 1:250){
#logit(pgx[l])<-beta[1]+beta[2]*gx[l]
#logit(pMx[l])<-beta[1]+beta[3]*Mx[l]
#logit(pRx[l])<-beta[1]+beta[4]*Rx[l]
#logit(psfrx[l])<-beta[1]+beta[5]*sfrx[l]
#logit(pgrx[l])<-beta[1]+beta[6]*grx[l]
# S galaxies
#logit(pgxS[l])<-beta[1]+beta[2]*gx[l]+beta[7]
#logit(pMxS[l])<-beta[1]+beta[3]*Mx[l]+beta[7]
#logit(pRxS[l])<-beta[1]+beta[4]*Rx[l]+beta[7]
#logit(psfrS[l])<-beta[1]+beta[5]*sfrx[l]+beta[7]
#logit(pgrS[l])<-beta[1]+beta[6]*grx[l]+beta[7]
#             }
}"
#params <- c("beta","pi","pgx","pMx","pRx","psfrx","pgrx",
#            "pgxS","pMxS","pRxS","psfrS","pgrS")        # Monitor these parameters.
params <- c("beta","pi")        # Monitor these parameters.
#inits0  <- function () {list(beta = rnorm(K, 0, 0.01))} # A function to generat initial values for mcmc
inits<-function(){list(beta=structure(.Data=c(rnorm(2*K,0,0.01)),.Dim=c(2,K)))}# A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
# Run mcmc
bin    = 10^4   # burn-in samples
ad     = 10^4   # Number of adaptive samples
s      = 3*10^4 # Number of samples for each chain
nc     = 3      # Number of mcmc
th     = 10     # Thinning value
jags.logit  <- run.jags(method="rjparallel",data = jags.data,inits = list(inits1,inits2,inits3),model=model,
n.chains = nc,adapt=ad,monitor=c(params),burnin=bin,thin=th,sample=s,summarise=FALSE,plots=FALSE)
list(inits1,inits2,inits3)
inits1=inits0();inits2=inits0();inits3=inits0()
inits1
inits0<-function(){list(beta=structure(.Data=c(rnorm(2*K,0,0.01)),.Dim=c(2,K)))}# A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
# Run mcmc
bin    = 10^4   # burn-in samples
ad     = 10^4   # Number of adaptive samples
s      = 3*10^4 # Number of samples for each chain
nc     = 3      # Number of mcmc
th     = 10     # Thinning value
jags.logit  <- run.jags(method="rjparallel",data = jags.data,inits = list(inits1,inits2,inits3),model=model,
n.chains = nc,adapt=ad,monitor=c(params),burnin=bin,thin=th,sample=s,summarise=FALSE,plots=FALSE)
list(inits1,inits2,inits3)
inits0<-function(){list(beta=structure(.Data=c(rnorm(2*K,0,0.01)),.Dim=c(K,2)))}# A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
inits0()
### Prior Specification and Likelihood function
model<-"model{
#1. Priors
#a.Normal
#beta~dmnorm(b0,B0)
#tau~dgamma(0.01,0.01)
for(k in 1:7){
for(j in 1:2){
beta[k,j] ~ dnorm(0,0.01)
}  # close J loop
}  # close K loop
#b.Jefreys priors for sparseness
#for(j in 1:K)   {
#lnTau[j] ~ dunif(-50, 50)
#TauM[j] <- exp(lnTau[j])
#beta[j] ~ dnorm(0, TauM[j])
#Ind[j] <- step(abs(beta[j]) - 0.05)
#}
#c.LASSO
#for(j in 1:K){
#beta[j]~ddexp(0,tau)
#}
#c1.prior for tau
#tau <-pow(sdBeta,-1)
#sdBeta ~ dgamma(0.01,0.01)
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i,1:J])
for (j in 1:J){
#logit(pi[i,j]) <-  eta[i]
#eta[i] <- inprod(beta[galtype[i]], X[i,])
logit(pi[i,j]) <- beta[1,j]+beta[2,j]*X[i,2]+beta[3,j]*X[i,3]+beta[4,j]*X[i,4]+beta[5,j]*X[i,5]+beta[6,j]*X[i,6]+
beta[7,j]*X[i,7]
}   # close J loop
}     # close N loop
#3.Probability for each variable
# E galaxies
#for(l in 1:250){
#logit(pgx[l])<-beta[1]+beta[2]*gx[l]
#logit(pMx[l])<-beta[1]+beta[3]*Mx[l]
#logit(pRx[l])<-beta[1]+beta[4]*Rx[l]
#logit(psfrx[l])<-beta[1]+beta[5]*sfrx[l]
#logit(pgrx[l])<-beta[1]+beta[6]*grx[l]
# S galaxies
#logit(pgxS[l])<-beta[1]+beta[2]*gx[l]+beta[7]
#logit(pMxS[l])<-beta[1]+beta[3]*Mx[l]+beta[7]
#logit(pRxS[l])<-beta[1]+beta[4]*Rx[l]+beta[7]
#logit(psfrS[l])<-beta[1]+beta[5]*sfrx[l]+beta[7]
#logit(pgrS[l])<-beta[1]+beta[6]*grx[l]+beta[7]
#             }
}"
#params <- c("beta","pi","pgx","pMx","pRx","psfrx","pgrx",
#            "pgxS","pMxS","pRxS","psfrS","pgrS")        # Monitor these parameters.
params <- c("beta","pi")        # Monitor these parameters.
#inits0  <- function () {list(beta = rnorm(K, 0, 0.01))} # A function to generat initial values for mcmc
inits0<-function(){list(beta=structure(.Data=c(rnorm(2*K,0,0.01)),.Dim=c(K,2)))}# A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
# Run mcmc
bin    = 10^4   # burn-in samples
ad     = 10^4   # Number of adaptive samples
s      = 3*10^4 # Number of samples for each chain
nc     = 3      # Number of mcmc
th     = 10     # Thinning value
jags.logit  <- run.jags(method="rjparallel",data = jags.data,inits = list(inits1,inits2,inits3),model=model,
n.chains = nc,adapt=ad,monitor=c(params),burnin=bin,thin=th,sample=s,summarise=FALSE,plots=FALSE)
# JAGS Code with Adaptive Shrinkage
#  Required libraries
library(rjags);library(ggmcmc);library(ggplot2);library(ggthemes);library(pander);library(Cairo);library(MASS);library(parallel)
library(scales);library(plyr);require(gdata);require(runjags);require(gdata);require(caret);require(pROC);require(plyr)
cl       <- makeCluster(3) # For parallel computing
# Read and format data
data     <- read.csv("..//data/sample_CRP02_sub.csv",header=TRUE,na.strings="")
data_cut <- data[,c("bpt","lgm_tot_p50","logM200_L","RprojLW_Rvir","sfr_tot_p50","color_gr","zoo")]
data2    <- na.omit(data_cut)
data2    <- data2[data2$lgm_tot_p50>0,]
data2    <- data2[which(data2$logM200_L>0),]
data2    <- data2[which(data2$RprojLW_Rvir>=0),]
data2    <- data2[which(data2$sfr_tot_p50>=-100),]
# Standardized variables
data2<-data.frame(bpt=data2$bpt,as.data.frame(scale(data2[,2:6])),zoo=data2$zoo)
trainIndex <- sample(1:nrow(data2),100)
data3      <- data2[trainIndex,]
#data3    <- subset(data3, bpt!="LINER")    # remove Liners
data3$bpt  <- revalue(data3$bpt,c("Star Forming"="0","Composite"="0",
"LINER"="1","Seyfert/LINER"="1","Star Fo"="0",
"Seyfert"="1","BLANK"="0"))
data_n     <- data3
data_n2    <- subset(data_n, zoo=="E" | zoo == "S")
galtype    <- match(data_n2$zoo,c("E", "S"))
galtype    <- galtype-1                 # 0 = E and 1 = S
X          <- model.matrix( ~ lgm_tot_p50 + logM200_L + RprojLW_Rvir +
sfr_tot_p50 + color_gr + galtype, data = data_n2) # Predictors
K          <- ncol(X)                   # Number of Predictors including the intercept
y          <- as.numeric(data_n2$bpt)-1 # Response variable (0/1)
n          <- length(y)                 # Sample size
# Grid of values for prediction
gx <- seq(1.75*min(X[,2]),1.75*max(X[,2]),length.out=250)
Mx <- seq(1.75*min(X[,2]),1.75*max(X[,3]),length.out=250)
Rx <- seq(1.75*min(X[,2]),1.75*max(X[,4]),length.out=250)
sfrx <- seq(1.75*min(X[,2]),1.75*max(X[,5]),length.out=250)
grx <-  seq(1.75*min(X[,2]),1.75*max(X[,6]),length.out=250)
jags.data  <- list(Y= y,N = n,X=X,b0 = rep(0,K),B0=diag(1e-4,K),gx=gx,Mx=Mx,
Rx=Rx,sfrx=sfrx, grx = grx )
# b0 and B0 are the mean vector and the inverse of the covariance matrix of prior distribution of the regression coefficients
### Prior Specification and Likelihood function
model<-"model{
#1. Priors
#a.Normal
beta~dmnorm(b0,B0)
#b.Jefreys priors for sparseness
#for(j in 1:K)   {
#lnTau[j] ~ dunif(-50, 50)
#TauM[j] <- exp(lnTau[j])
#beta[j] ~ dnorm(0, TauM[j])
#Ind[j] <- step(abs(beta[j]) - 0.05)
#}
#c.LASSO
#for(j in 1:K){
#beta[j]~ddexp(0,tau)
#}
#c1.prior for tau
#tau <-pow(sdBeta,-1)
#sdBeta ~ dgamma(0.01,0.01)
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
#3.Probability for each variable
# E galaxies
for(l in 1:250){
logit(pgx[l])<-beta[1]+beta[2]*gx[l]
logit(pMx[l])<-beta[1]+beta[3]*Mx[l]
logit(pRx[l])<-beta[1]+beta[4]*Rx[l]
logit(psfrx[l])<-beta[1]+beta[5]*sfrx[l]
logit(pgrx[l])<-beta[1]+beta[6]*grx[l]
# S galaxies
logit(pgxS[l])<-beta[1]+beta[2]*gx[l]+beta[7]
logit(pMxS[l])<-beta[1]+beta[3]*Mx[l]+beta[7]
logit(pRxS[l])<-beta[1]+beta[4]*Rx[l]+beta[7]
logit(psfrS[l])<-beta[1]+beta[5]*sfrx[l]+beta[7]
logit(pgrS[l])<-beta[1]+beta[6]*grx[l]+beta[7]
}
}"
params <- c("beta","pi","pgx","pMx","pRx","psfrx","pgrx",
"pgxS","pMxS","pRxS","psfrS","pgrS")        # Monitor these parameters.
inits0  <- function () {list(beta = rnorm(K, 0, 0.01))} # A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
# Run mcmc
bin    = 10^4   # burn-in samples
ad     = 10^4   # Number of adaptive samples
s      = 3*10^4 # Number of samples for each chain
nc     = 3      # Number of mcmc
th     = 10     # Thinning value
jags.logit  <- run.jags(method="rjparallel",data = jags.data,inits = list(inits1,inits2,inits3),model=model,
n.chains = nc,adapt=ad,monitor=c(params),burnin=bin,thin=th,sample=s,summarise=FALSE,plots=FALSE)
setwd("~/Dropbox/artigos/Meusartigos/IAA-WGC/Github/Logit_Mar/script")
# JAGS Code with Adaptive Shrinkage
#  Required libraries
library(rjags);library(ggmcmc);library(ggplot2);library(ggthemes);library(pander);library(Cairo);library(MASS);library(parallel)
library(scales);library(plyr);require(gdata);require(runjags);require(gdata);require(caret);require(pROC);require(plyr)
cl       <- makeCluster(3) # For parallel computing
# Read and format data
data     <- read.csv("..//data/sample_CRP02_sub.csv",header=TRUE,na.strings="")
data_cut <- data[,c("bpt","lgm_tot_p50","logM200_L","RprojLW_Rvir","sfr_tot_p50","color_gr","zoo")]
data2    <- na.omit(data_cut)
data2    <- data2[data2$lgm_tot_p50>0,]
data2    <- data2[which(data2$logM200_L>0),]
data2    <- data2[which(data2$RprojLW_Rvir>=0),]
data2    <- data2[which(data2$sfr_tot_p50>=-100),]
# Standardized variables
data2<-data.frame(bpt=data2$bpt,as.data.frame(scale(data2[,2:6])),zoo=data2$zoo)
trainIndex <- sample(1:nrow(data2),100)
data3      <- data2[trainIndex,]
#data3    <- subset(data3, bpt!="LINER")    # remove Liners
data3$bpt  <- revalue(data3$bpt,c("Star Forming"="0","Composite"="0",
"LINER"="1","Seyfert/LINER"="1","Star Fo"="0",
"Seyfert"="1","BLANK"="0"))
data_n     <- data3
#data_n2    <- subset(data_n, zoo=="E" | zoo == "S")
data_n2    <- subset(data_n, zoo=="E")
#galtype    <- match(data_n2$zoo,c("E", "S"))
#galtype    <- galtype-1                 # 0 = E and 1 = S
X          <- model.matrix( ~ lgm_tot_p50 + logM200_L + RprojLW_Rvir +
sfr_tot_p50 + color_gr, data = data_n2) # Predictors
K          <- ncol(X)                   # Number of Predictors including the intercept
y          <- as.numeric(data_n2$bpt)-1 # Response variable (0/1)
n          <- length(y)                 # Sample size
trainIndex <- sample(1:nrow(data2),100)
data3      <- data2[trainIndex,]
#data3    <- subset(data3, bpt!="LINER")    # remove Liners
data3$bpt  <- revalue(data3$bpt,c("Star Forming"="0","Composite"="0",
"LINER"="1","Seyfert/LINER"="1","Star Fo"="0",
"Seyfert"="1","BLANK"="0"))
data_n     <- data3
#data_n2    <- subset(data_n, zoo=="E" | zoo == "S")
data_n2    <- subset(data_n, zoo=="E")
#galtype    <- match(data_n2$zoo,c("E", "S"))
#galtype    <- galtype-1                 # 0 = E and 1 = S
X          <- model.matrix( ~ lgm_tot_p50 + logM200_L + RprojLW_Rvir +
sfr_tot_p50 + color_gr, data = data_n2) # Predictors
K          <- ncol(X)                   # Number of Predictors including the intercept
y          <- as.numeric(data_n2$bpt)-1 # Response variable (0/1)
n          <- length(y)                 # Sample size
# Grid of values for prediction
gx <- seq(1.75*min(X[,2]),1.75*max(X[,2]),length.out=250)
Mx <- seq(1.75*min(X[,2]),1.75*max(X[,3]),length.out=250)
Rx <- seq(1.75*min(X[,2]),1.75*max(X[,4]),length.out=250)
sfrx <- seq(1.75*min(X[,2]),1.75*max(X[,5]),length.out=250)
grx <-  seq(1.75*min(X[,2]),1.75*max(X[,6]),length.out=250)
jags.data  <- list(Y= y,N = n,X=X,b0 = rep(0,K),B0=diag(1e-4,K),gx=gx,Mx=Mx,
Rx=Rx,sfrx=sfrx, grx = grx )
# b0 and B0 are the mean vector and the inverse of the covariance matrix of prior distribution of the regression coefficients
### Prior Specification and Likelihood function
model<-"model{
#1. Priors
#a.Normal
beta~dmnorm(b0,B0)
#b.Jefreys priors for sparseness
#for(j in 1:K)   {
#lnTau[j] ~ dunif(-50, 50)
#TauM[j] <- exp(lnTau[j])
#beta[j] ~ dnorm(0, TauM[j])
#Ind[j] <- step(abs(beta[j]) - 0.05)
#}
#c.LASSO
#for(j in 1:K){
#beta[j]~ddexp(0,tau)
#}
#c1.prior for tau
#tau <-pow(sdBeta,-1)
#sdBeta ~ dgamma(0.01,0.01)
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
#3.Probability for each variable
# E galaxies
for(l in 1:250){
logit(pgx[l])<-beta[1]+beta[2]*gx[l]
logit(pMx[l])<-beta[1]+beta[3]*Mx[l]
logit(pRx[l])<-beta[1]+beta[4]*Rx[l]
logit(psfrx[l])<-beta[1]+beta[5]*sfrx[l]
logit(pgrx[l])<-beta[1]+beta[6]*grx[l]
# S galaxies
logit(pgxS[l])<-beta[1]+beta[2]*gx[l]+beta[7]
logit(pMxS[l])<-beta[1]+beta[3]*Mx[l]+beta[7]
logit(pRxS[l])<-beta[1]+beta[4]*Rx[l]+beta[7]
logit(psfrS[l])<-beta[1]+beta[5]*sfrx[l]+beta[7]
logit(pgrS[l])<-beta[1]+beta[6]*grx[l]+beta[7]
}
}"
params <- c("beta","pi","pgx","pMx","pRx","psfrx","pgrx",
"pgxS","pMxS","pRxS","psfrS","pgrS")        # Monitor these parameters.
inits0  <- function () {list(beta = rnorm(K, 0, 0.01))} # A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
# Run mcmc
bin    = 10^4   # burn-in samples
ad     = 10^4   # Number of adaptive samples
s      = 3*10^4 # Number of samples for each chain
nc     = 3      # Number of mcmc
th     = 10     # Thinning value
jags.logit  <- run.jags(method="rjparallel",data = jags.data,inits = list(inits1,inits2,inits3),model=model,
n.chains = nc,adapt=ad,monitor=c(params),burnin=bin,thin=th,sample=s,summarise=FALSE,plots=FALSE)
X
data_n     <- data3
#data_n2    <- subset(data_n, zoo=="E" | zoo == "S")
data_n2    <- subset(data_n, zoo=="E")
#galtype    <- match(data_n2$zoo,c("E", "S"))
#galtype    <- galtype-1                 # 0 = E and 1 = S
X          <- model.matrix( ~ lgm_tot_p50 + logM200_L + RprojLW_Rvir +
sfr_tot_p50 + color_gr, data = data_n2) # Predictors
K          <- ncol(X)                   # Number of Predictors including the intercept
y          <- as.numeric(data_n2$bpt)-1 # Response variable (0/1)
n          <- length(y)                 # Sample size
# Grid of values for prediction
gx <- seq(1.75*min(X[,2]),1.75*max(X[,2]),length.out=250)
Mx <- seq(1.75*min(X[,2]),1.75*max(X[,3]),length.out=250)
Rx <- seq(1.75*min(X[,2]),1.75*max(X[,4]),length.out=250)
sfrx <- seq(1.75*min(X[,2]),1.75*max(X[,5]),length.out=250)
grx <-  seq(1.75*min(X[,2]),1.75*max(X[,6]),length.out=250)
jags.data  <- list(Y= y,N = n,X=X,b0 = rep(0,K),B0=diag(1e-4,K),gx=gx,Mx=Mx,
Rx=Rx,sfrx=sfrx, grx = grx )
# b0 and B0 are the mean vector and the inverse of the covariance matrix of prior distribution of the regression coefficients
### Prior Specification and Likelihood function
model<-"model{
#1. Priors
#a.Normal
beta~dmnorm(b0,B0)
#b.Jefreys priors for sparseness
#for(j in 1:K)   {
#lnTau[j] ~ dunif(-50, 50)
#TauM[j] <- exp(lnTau[j])
#beta[j] ~ dnorm(0, TauM[j])
#Ind[j] <- step(abs(beta[j]) - 0.05)
#}
#c.LASSO
#for(j in 1:K){
#beta[j]~ddexp(0,tau)
#}
#c1.prior for tau
#tau <-pow(sdBeta,-1)
#sdBeta ~ dgamma(0.01,0.01)
#2. Likelihood
for(i in 1:N){
Y[i] ~ dbern(pi[i])
logit(pi[i]) <-  eta[i]
eta[i] <- inprod(beta[], X[i,])
}
#3.Probability for each variable
# E galaxies
for(l in 1:250){
logit(pgx[l])<-beta[1]+beta[2]*gx[l]
logit(pMx[l])<-beta[1]+beta[3]*Mx[l]
logit(pRx[l])<-beta[1]+beta[4]*Rx[l]
logit(psfrx[l])<-beta[1]+beta[5]*sfrx[l]
logit(pgrx[l])<-beta[1]+beta[6]*grx[l]
# S galaxies
#logit(pgxS[l])<-beta[1]+beta[2]*gx[l]+beta[7]
#logit(pMxS[l])<-beta[1]+beta[3]*Mx[l]+beta[7]
#logit(pRxS[l])<-beta[1]+beta[4]*Rx[l]+beta[7]
#logit(psfrS[l])<-beta[1]+beta[5]*sfrx[l]+beta[7]
#logit(pgrS[l])<-beta[1]+beta[6]*grx[l]+beta[7]
}
}"
params <- c("beta","pi","pgx","pMx","pRx","psfrx","pgrx",
"pgxS","pMxS","pRxS","psfrS","pgrS")        # Monitor these parameters.
inits0  <- function () {list(beta = rnorm(K, 0, 0.01))} # A function to generat initial values for mcmc
inits1=inits0();inits2=inits0();inits3=inits0()         # Generate initial values for three chains
# Run mcmc
bin    = 10^4   # burn-in samples
ad     = 10^4   # Number of adaptive samples
s      = 3*10^4 # Number of samples for each chain
nc     = 3      # Number of mcmc
th     = 10     # Thinning value
jags.logit  <- run.jags(method="rjparallel",data = jags.data,inits = list(inits1,inits2,inits3),model=model,
n.chains = nc,adapt=ad,monitor=c(params),burnin=bin,thin=th,sample=s,summarise=FALSE,plots=FALSE)
summary(jags.logit)
jagssamples <- as.mcmc.list(jags.logit)
ggs(jagssamples,family="beta")
plotbeta<-ggs_caterpillar(G1)+theme_hc()+
theme(legend.position="none",plot.title = element_text(hjust=0.5),
axis.title.y=element_text(vjust=0.75),axis.text.x=element_text(size=18),
axis.text.y=element_text(size=18),
strip.text.x=element_text(size=25),
axis.title.x=element_text(vjust=-0.25),
text = element_text(size=20),axis.title.x=element_text(size=rel(1)))+
geom_vline(xintercept=0,linetype="dashed",colour=c("#034e7b")) +
aes(color="#034e7b")+ylab("")+
scale_y_discrete(breaks=c("beta[1]", "beta[2]", "beta[3]","beta[4]","beta[5]","beta[6]","beta[7]"),
labels=c(expression(beta[1]),expression(beta[2]),expression(beta[3]),
expression(beta[4]),expression(beta[5]),expression(beta[6]),expression(beta[7])))
G1<-ggs(jagssamples,family="beta")
plotbeta<-ggs_caterpillar(G1)+theme_hc()+
theme(legend.position="none",plot.title = element_text(hjust=0.5),
axis.title.y=element_text(vjust=0.75),axis.text.x=element_text(size=18),
axis.text.y=element_text(size=18),
strip.text.x=element_text(size=25),
axis.title.x=element_text(vjust=-0.25),
text = element_text(size=20),axis.title.x=element_text(size=rel(1)))+
geom_vline(xintercept=0,linetype="dashed",colour=c("#034e7b")) +
aes(color="#034e7b")+ylab("")+
scale_y_discrete(breaks=c("beta[1]", "beta[2]", "beta[3]","beta[4]","beta[5]","beta[6]","beta[7]"),
labels=c(expression(beta[1]),expression(beta[2]),expression(beta[3]),
expression(beta[4]),expression(beta[5]),expression(beta[6]),expression(beta[7])))
plotbeta
data_n2[1,]
data_n2$zoo
trainIndex <- sample(1:nrow(data2),2000)
data3      <- data2[trainIndex,]
#data3    <- subset(data3, bpt!="LINER")    # remove Liners
data3$bpt  <- revalue(data3$bpt,c("Star Forming"="0","Composite"="0",
"LINER"="1","Seyfert/LINER"="1","Star Fo"="0",
"Seyfert"="1","BLANK"="0"))
data_n     <- data3
data_n2    <- subset(data_n, zoo=="E")
data_n2$zoo
data_n2    <- subset(data_n, zoo=="S")
data_n2$zoo
data_n2    <- subset(data_n, zoo=="E")
data_n2 [1,]
data_nE   <- subset(data_n, zoo=="E")
data_nS   <- subset(data_n, zoo=="S")
glm(bpt~sfr_tot_p50,data=data_nE )
as.numeric(bpt)-1
data_nE$bpt
as.numeric(data_nE$bpt)
as.numeric(data_nE$bpt)-1
data_nS$bpt<-as.numeric(data_nS$bpt)-1
data_nE$bpt<-as.numeric(data_nE$bpt)-1
glm(bpt~sfr_tot_p50,data=data_nE)
glm(bpt~sfr_tot_p50,data=data_nS)
glm(bpt~sfr_tot_p50+sfr_tot_p50^2,data=data_nS)
glm(bpt~sfr_tot_p50+sfr_tot_p50*sfr_tot_p50,data=data_nS)
b <- gam(bpt~s(sfr_tot_p50),data=data_nS)
summary(b)
require(mgcv)
b <- gam(bpt~s(sfr_tot_p50),data=data_nS)
summary(b)
plot(b)
vis.gam(b,color="heat",theta=30,phi=30)
fit <- predict(model , se = TRUE )$fit
se <- predict( model , se = TRUE)$se.fit
lcl <- fit - 1.96 * se
ucl <- fit + 1.96 * se
plot( 0 , type = "n" , bty = "n" , xlab = "The x-axis lable" , ylab = "The y-axis lable" , main = "The Title" , xlim = c(0,5) , ylim = c(0,3) )
model<- gam(bpt~s(sfr_tot_p50),data=data_nS)
fit <- predict(model , se = TRUE )$fit
se <- predict( model , se = TRUE)$se.fit
lcl <- fit - 1.96 * se
ucl <- fit + 1.96 * se
plot( 0 , type = "n" , bty = "n" , xlab = "The x-axis lable" , ylab = "The y-axis lable" , main = "The Title" , xlim = c(0,5) , ylim = c(0,3) )
se <- predict( model , se = TRUE)$se.fit
lcl <- fit - 1.96 * se
ucl <- fit + 1.96 * se
fit
plot( 0 , type = "n" , bty = "n" , xlab = "The x-axis lable" , ylab = "The y-axis lable" , main = "The Title" , xlim = c(0,5) , ylim = c(0,3) )
i.for <- order( MyDataset$MyCovariate )
i.back <- order( MyDataset$MyCovariate , decreasing = TRUE )
x.polygon <- c( MyDataset$MyCovariate[i.for] , MyDataset$MyCovariate[i.back] )
y.polygon <- c( ucl[i.for] , lcl[i.back] )
polygon( x.polygon , y.polygon , col = "#A6CEE3" , border = NA )
lines( MyDataset$MyCovariate[i.for] , fit[i.for], col = "#1F78B4" , lwd = 3 )
fit
plot(sfr_tot_p50,fit )
plot(sfr_tot_p50,data_n2$fit)
plot(data_nSsfr_tot_p50,fit)
plot(data_nS$sfr_tot_p50,fit)
data_nS[1,]
glm(bpt~lgm_tot_p50+logM200_L+RprojLW_Rvir+sfr_tot_p50+color_gr,data=data_nS)
glm(bpt~lgm_tot_p50+logM200_L+RprojLW_Rvir+sfr_tot_p50+color_gr,data=data_nE)
glm(bpt~lgm_tot_p50+logM200_L+RprojLW_Rvir+sfr_tot_p50+color_gr,family=binomial("logit"),data=data_nS)
fitS<-glm(bpt~lgm_tot_p50+logM200_L+RprojLW_Rvir+sfr_tot_p50+color_gr,family=binomial("logit"),data=data_nS)
fitE<-glm(bpt~lgm_tot_p50+logM200_L+RprojLW_Rvir+sfr_tot_p50+color_gr,family=binomial("logit"),data=data_nE)
summary(fitS)
summary(fitE)
data(cars)
cars
cars[1,]
cars[1:10,]
write.table(cars)
write.table(cars,"cars.txt",sep="")
write.matrix(cars,"cars.txt",sep="",header=T)
require(MASS)
write.matrix(cars,"cars.txt")
